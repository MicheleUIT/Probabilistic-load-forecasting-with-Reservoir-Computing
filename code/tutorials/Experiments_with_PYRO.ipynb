{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "import numpy as np\n",
    "\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYRO\n",
    "[Getting started page](https://pyro.ai/examples/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Given some observations $x$, latent RV $z$, and paramenter $\\theta$, the link between them is called (probabilistic) **model**, with pdf $$p_\\theta(x,z)=p_\\theta(x|z)p_\\theta(z)$$\n",
    "where $p_\\theta(x|z)$ is the *likelihood*, $p_\\theta(z)$ is the *prior*.\n",
    "\n",
    "Things that we may be interested in are:\n",
    "- inferring something about $z$ from data, i.e., the *posterior* $p_\\theta(z|x)=\\frac{p_\\theta(x,z)}{\\int dz p_\\theta(x,z)}$\n",
    "- how well the model describes data, i.e., the *marginal* $p_\\theta(x)=\\int dz p_\\theta(x,z)$\n",
    "- predict new data from the *posterior predictive distribution* $p_\\theta(x'|x)=\\int dz p_\\theta(x'|z)p_\\theta(z|x)$\n",
    "- learning the parameter $\\theta$ that best explain data, $\\theta_\\text{max}=\\argmax_\\theta p_\\theta(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data with 6 observed heads and 4 observed tails\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(4):\n",
    "    data.append(torch.tensor(0.0))\n",
    "data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"284pt\" height=\"157pt\"\n viewBox=\"0.00 0.00 284.24 157.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 153)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-153 280.24,-153 280.24,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_observe_data</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"17.74,-8 17.74,-83 105.74,-83 105.74,-8 17.74,-8\"/>\n<text text-anchor=\"middle\" x=\"61.74\" y=\"-15.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">observe_data</text>\n</g>\n<!-- latent_fairness -->\n<g id=\"node1\" class=\"node\">\n<title>latent_fairness</title>\n<ellipse fill=\"white\" stroke=\"black\" cx=\"61.74\" cy=\"-130\" rx=\"61.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"61.74\" y=\"-126.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">latent_fairness</text>\n</g>\n<!-- obs -->\n<g id=\"node2\" class=\"node\">\n<title>obs</title>\n<ellipse fill=\"grey\" stroke=\"black\" cx=\"61.74\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"61.74\" y=\"-53.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">obs</text>\n</g>\n<!-- latent_fairness&#45;&gt;obs -->\n<g id=\"edge1\" class=\"edge\">\n<title>latent_fairness&#45;&gt;obs</title>\n<path fill=\"none\" stroke=\"black\" d=\"M61.74,-111.81C61.74,-103.79 61.74,-94.05 61.74,-85.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"65.24,-85.03 61.74,-75.03 58.24,-85.03 65.24,-85.03\"/>\n</g>\n<!-- distribution_description_node -->\n<g id=\"node3\" class=\"node\">\n<title>distribution_description_node</title>\n<text text-anchor=\"start\" x=\"149.24\" y=\"-133.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">latent_fairness ~ Beta</text>\n<text text-anchor=\"start\" x=\"149.24\" y=\"-118.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">obs ~ Bernoulli</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x297c9228eb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(data):\n",
    "    # define the hyperparameters that control the Beta prior\n",
    "    alpha0 = torch.tensor(10.0)\n",
    "    beta0 = torch.tensor(10.0)\n",
    "    # sample f from the Beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # vectorized plate over the observed data\n",
    "    with pyro.plate('observe_data'):\n",
    "        # likelihood Bernoulli(f)\n",
    "        pyro.sample('obs', dist.Bernoulli(f), obs=data)\n",
    "\n",
    "pyro.render_model(model, model_args=(data,), render_distributions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using plate tells Pyro that the observations are indipendent (conditioned on the latent variable), but it also allow to do subsampling. Notice that if data is on GPU, you should pass a \"device\" argument to plate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide\n",
    "A **guide** is the variational distribution $q_\\phi(z)$ that approximates the posterior $p_\\theta(z|x)$. It does not contain data, unlike the model, but it must have all the latent variables that the model has (the correspondence is guaranteed by their **names**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(data):\n",
    "    # register the two variational parameters with Pyro.\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the guide above doesn't have the \"obs\" RV, but it could have a \"plate\" in more complex scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVI\n",
    "In order to do variational inference PYRO uses SVI class, which needs: a model, a guide and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.032658100128174\n",
      "Loss: 7.071733474731445\n",
      "Loss: 7.062897801399231\n",
      "Loss: 7.093270421028137\n",
      "Loss: 7.069697618484497\n",
      "Loss: 7.077534437179565\n",
      "Loss: 7.073229670524597\n",
      "Loss: 7.075173497200012\n",
      "Loss: 7.040586233139038\n",
      "Loss: 7.074746489524841\n"
     ]
    }
   ],
   "source": [
    "# set up the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 5000\n",
    "losses = []\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(data) # step ensures that data is passed to both model and guide\n",
    "    losses.append(loss)\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.537 +- 0.089\n"
     ]
    }
   ],
   "source": [
    "# grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "# here we use some facts about the Beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nBased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple scenario we can analytically compute the true posterior using Bayes' theorem:\n",
    "- The latent variable $z$ is defined within the model and its prior is a Beta distribution, i.e., $$p(z;\\alpha,\\beta)=\\frac{1}{B(\\alpha,\\beta)}z^{\\alpha-1}(1-z)^{\\beta-1},$$ where $B(\\alpha,\\beta)$ is the normalizing factor.\n",
    "- The likelihood is defined within the model as well and it's a Bernoulli, $p(x|z)=z^x(1-z)^{1-x}$.\n",
    "\n",
    "So the true posterior is \n",
    "$$\n",
    "\\begin{align}\n",
    "    p(z|x) &\\propto \\prod_{i=1}^N \\, p(z;\\alpha,\\beta)p(x_i|z) \\propto \\\\\n",
    "        &\\propto \\prod_i \\, z^{\\alpha-1}(1-z)^{\\beta-1}z^x_i(1-z)^{1-x_i} = \\\\\n",
    "        &= z^{\\alpha-1+\\sum_i x_i}(1-z)^{N+\\beta-1-\\sum_i x_i} \n",
    "\\end{align}\n",
    "$$\n",
    "which is again a Beta distribution (so our guide is not an _approximation_ of the true distribution, that explains why the loss doesn't really decrease much) with coefficients\n",
    "$$\n",
    "    \\tilde{\\alpha} = \\alpha +\\sum_{i=1}^N\\,x_i = 10 + 6 = 16\n",
    "$$\n",
    "$$\n",
    "    \\tilde{\\beta} = N + \\beta - \\sum_{i=1}^N\\,x_i = 10 + 10 - 6 = 14\n",
    "$$\n",
    "which give\n",
    "$$\n",
    "    \\text{mean} = \\frac{\\tilde{\\alpha}}{\\tilde{\\alpha}+\\tilde{\\beta}} = \\frac{16}{30} = 0.5\\bar{3}\n",
    "$$\n",
    "$$\n",
    "    \\text{std} = \\frac{1}{\\tilde{\\alpha}+\\tilde{\\beta}}\\sqrt{\\frac{\\tilde{\\alpha}\\tilde{\\beta}}{\\tilde{\\alpha}+\\tilde{\\beta}+1}} \\simeq 0.09\n",
    "$$\n",
    "They are compatible with what found with SVI\n",
    "$$\n",
    "    \\lambda = \\frac{|0.5\\bar{3}-0.537|}{\\sqrt{0.089^2+0.09^2}} \\simeq 0.03 < 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# sum of x\n",
    "sum = torch.tensor(data).sum()\n",
    "print(sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('uncertainty')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60557ac213c4f904567501e30163cbfc56b9c76312e53cab0b1cef6c972a9f33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
